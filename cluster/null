#
# Copyright (C) 2018 Heinrich-Heine-Universitaet Duesseldorf, Institute of Computer Science, Department Operating Systems
#
# This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>
#

#!/bin/bash

#
# This module is not an implementation of a cluster system but provides all
# functions that must be implemented for the cluster abstraction layer.
# Some functions or feature might not be available or necessary on your
# cluster setup.
#
# You should checkout the existing implementations as well before starting your
# own. Either there is already one that supports your setup or you can benefit
# from it by copying great parts and tweaking it to match your setup.
#

#################################################
# Public API for deploy scripts
#################################################

##
# Return the location where to put any output data of the deployment
# (logfiles, configurations, job scripts etc)
#
# $1: User name of the account to use on the target cluster
# ret stdout: Base location (depending on the user) where to put deployment
#             output
##
cdepl_cluster_get_base_path_deploy_out()
{
	local user=$1

	# Default impl
	echo "/home/$user"
}

##
# Allocate the specified number of nodes
#
# Either keep track of the number of nodes allocated, actually execute
# allocation here to physically claim the nodes or prepare to create a job
# script if your cluster runs a job system.
#
# $1: Number of nodes to allocate
##
cdepl_cluster_node_alloc()
{
	local num_nodes=$1

	# stub
}

##
# Get the number of allocated nodes
#
# ret stdout: Number of allocated nodes
##
cdepl_cluster_get_alloc_node_count()
{
	# stub
}

##
# Allocate all hardware resources available on a particular node
#
# If multiple jobs can be executed on the same node by the job system, you
# can specify exclusivity here. Note: Not necessarily supported or required
# to be implemented for every cluster
#
# $1: Node id of the node to allocate
##
cdepl_cluster_node_excl()
{
	local node=$1

	# stub
}

##
# Limit the application(s) getting started on the target node to a single cpu 
# socket. Depending on the application, this might be very crucial for 
# performance if the target application is not (well) optimized for multi
# socket CPUs. Ensure your cluster enforces this setting if it is has nodes
# with multi socket hardware.
#
# $1: Node id of the node to apply this setting to
##
cdepl_cluster_node_cpu_limit_single_socket()
{
	local node=$1
}

##
# Allocate a specific number of CPUs/cores on the target node
#
# If this feature can be implemented with your cluster, ensure that you check
# with the amount of available resources (if possible)
#
# $1: Node id of the node to allocate the resource on
# $2: Number of CPUs/cores to allocate
##
cdepl_cluster_node_cpus()
{
	local node=$1
	local cpus=$2

	# stub
}

##
# Allocate a specific amount of RAM on the target node
#
# If this feature can be implemented with your cluster, ensure that you check
# with the amount of available resources (if possible)
#
# $1: Node id of the node to allocate the resource on
# $2: Amount of memory (in MB) to allocate
##
cdepl_cluster_node_mem()
{
	local node=$1
	local mem=$2

	# stub
}

##
# Allocate a specific network type on the target node
#
# If this feature can be implemented with your cluster, ensure that you check
# if the target network hardware is available
#
# $1: Node id of the node to allocate the resource on
# $2: Network type to allocate: eth, ib
##
cdepl_cluster_node_network()
{
	local node=$1
	local net=$2

	# stub
}

##
# Resolve a hostname of a cluster node to the corresponding IPv4 address
#
# $1: hostname to resolve
# ret stdout: IPv4 address or empty string in non resolvable
##
cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

	# stub
}

##
# Resolve an abstract node id of a cluster node to the corresponding IPv4 address
#
# $1: node id to resolve
# ret stdout: IPv4 address or empty string in non resolvable
##
cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	# stub
}

##
# Resolve an abstract node id of a cluster node to the hostname of the node
#
# $1: node id to resolve
# ret stdout: hostname of the node or empty string in non resolvable
##
cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	# stub
}

##
# Send a command to a cluster node
#
# $1: node id of the node to send the command to
# $2: (ssh) command to send
# $3: An optional space separated list of required environments/dependencies
#     for this command, e.g. java/1.8 gcc/6.1
##
cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"
	local required_env="$3"

	# stub
}

##
# Get all the log files from the nodes on a setup which doesn't use NFS.
#
# $1 Target destination to copy them to (local on the machine you run cdepl 
#    from)
# $2 The output path for log files, configs etc of the deployment on the nodes
##
cdepl_cluster_gather_log_files()
{
	local path="$1"
	local cur_path="$2"

	# stub
}

##
# Upload a file or directory to the remote cluster.
#
# If you are using NFS, you have to upload the file to a single target node or
# the login node. If no NFS is available, ensure that the file is available on
# all nodes envolved in the current delpoyment.
#
# $1 Local path to file or directory to copy to remote
# $2 Remote path to copy to
# $3 Optional: Recursive copy. If directory, set this argument (e.g. 1)
##
cdepl_cluster_upload_to_remote()
{
	local local_path=$1
	local remote_path=$2
	local recursive=$3

	# stub
}

##
# Download a file or directory from the remote cluster.
#
# $1 Remote path to file or directory to download from remote
# $2 Local path to download to
# $3 Optional: Recursive copy. If directory, set this argument (e.g. 1)
##
cdepl_cluster_download_from_remote()
{
	local remote_path=$1
	local local_path=$2
	local recursive=$3

	# stub
}

##
# Send a command for setting up files to all cluster nodes
#
# $1: (ssh) command to send
# $2: An optional space separated list of required environments/dependencies
#     for this command, e.g. java/1.8 gcc/6.1
##
cdepl_cluster_file_system_cmd()
{
	local cmd="$1"
	local required_env="$2"

	# stub
}

cdepl_cluster_login_cmd()
{
	local cmd="$1"
	local required_env=""
	cdepl_cluster_node_cmd 0 "$cmd"
}

##
# Tell the caller if the current cluster allows executing commands using sudo
#
# $ret Empty string if not allowed and non empty string if allowed (e.g. "1")
##
cdepl_cluster_allows_sudo()
{
	echo ""
}

#################################################
# Callbacks for cdepl
#################################################

##
# "Constructor" of the cluster module
#
# Called right after the module is loaded into the cdepl environment
#
# $1: Name of the user to use when logging into the cluster
# ...: Further optional arguments that can be passed to cdepl_cluster_init
##
_cdepl_cluster_on_init()
{
	local cluster_user=$1

	util_log_debug "[null] on_init"
}

##
# Callback on node setup and allocation phase finish
##
_cdepl_cluster_on_node_setup_finish()
{
	util_log_debug "[null] on_node_setup_finish"
}

##
# Callback before deployment starts
##
_cdepl_cluster_before_deploy()
{
	util_log_debug "[null] before_deploy"
}

##
# Callback right after deployment finished
##
_cdepl_cluster_after_deploy()
{
	util_log_debug "[null] after_deploy"
}

##
# Callback right before cleanup
##
_cdepl_cluster_before_cleanup()
{
	util_log_debug "[null] before_cleanup"
}

##
# Callback right after cleanup
##
_cdepl_cluster_after_cleanup()
{
	util_log_debug "[null] after_cleanup"
}
