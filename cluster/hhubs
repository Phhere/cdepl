#
# Copyright (C) 2018 Heinrich-Heine-Universitaet Duesseldorf, Institute of Computer Science, Department Operating Systems
#
# This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>
#

#!/bin/bash

#
# Cluster module for the private cluster of the operating systems research
# group of the Heinrich-Heine-University Duesseldorf
#

# Heterogeneous hardware, 3 groups
readonly __HHUBS_MAX_NUM_NODES=24
readonly __HHUBS_MAX_NUM_CORES=6
readonly __HHUBS_MAX_RAM=65536

# 4 cores, 32 GB RAM, InfiniBand
readonly __HHUBS_NODE_GROUP_1="node49 node50 node51 node52 node53 node54 node55 node56"
readonly __HHUBS_TOTAL_NODES_GROUP_1=8
readonly __HHUBS_NODE_GROUP_1_MAX_CORES=4
readonly __HHUBS_NODE_GROUP_1_MAX_RAM=32768
readonly __HHUBS_NODE_GROUP_1_MAX_NET="ib"

# 4 cores, 16 GB RAM
readonly __HHUBS_NODE_GROUP_2="node57 node58 node59 node60 node61 node62 node63 node64"
readonly __HHUBS_TOTAL_NODES_GROUP_2=8
readonly __HHUBS_NODE_GROUP_2_MAX_CORES=4
readonly __HHUBS_NODE_GROUP_2_MAX_RAM=16384
readonly __HHUBS_NODE_GROUP_2_MAX_NET="eth"

# 6 cores, 64 GB RAM, InfiniBand
readonly __HHUBS_NODE_GROUP_3="node65 node66 node67 node68 node69 node70 node71 node72"
readonly __HHUBS_TOTAL_NODES_GROUP_3=8
readonly __HHUBS_NODE_GROUP_3_MAX_CORES=6
readonly __HHUBS_NODE_GROUP_3_MAX_RAM=65536
readonly __HHUBS_NODE_GROUP_3_MAX_NET="ib"

__HHUBS_SOLLIPULLI_MODE=0
__HHUBS_SOLLIPULLI_USER=""

__HHUBS_TOTAL_NODES=""

# Init with minimums 0 and Ethernet
__HHUBS_NODES_CPUS=(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
__HHUBS_NODES_MEM=(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
__HHUBS_NODES_NET=("" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "")

__HHUBS_NODE_MAP=""

cdepl_cluster_get_base_path_deploy_out()
{
	local user=$1

	echo "/home/$user"
}

cdepl_cluster_node_alloc()
{
	local num_nodes=$1

	if [ "$num_nodes" -gt "$__HHUBS_MAX_NUM_NODES" ]; then
		util_log_error_and_exit "[hhubs] Not enough nodes available to allocate $num_nodes nodes, max $__HHUBS_MAX_NUM_NODES nodes"
	fi

	__HHUBS_TOTAL_NODES="$num_nodes"
}

cdepl_cluster_get_alloc_node_count()
{
	echo "$__HHUBS_TOTAL_NODES"
}

cdepl_cluster_node_excl()
{
	local node=$1

	# Nodes are always exclusive, ignore
}

cdepl_cluster_node_cpu_limit_single_socket()
{
	local node=$1

	# No need to limit to single socket, cluster does not have dual socket hardware
}

cdepl_cluster_node_cpus()
{
	local node=$1
	local cpus=$2

	if [ "$cpus" -gt "$__HHUBS_MAX_NUM_CORES" ]; then
		util_log_error_and_exit "[hhubs] No instance with $cpus cpus available, max cpus $__HHUBS_MAX_NUM_CORES"
	fi

	__HHUBS_NODES_CPUS[$node]="$cpus"
}

cdepl_cluster_node_mem()
{
	local node=$1
	local mem=$2

	if [ "$mem" -gt "$__HHUBS_MAX_RAM" ]; then
		util_log_error_and_exit "[hhubs] No instance with $mem mb memory available, max mem $__HHUBS_MAX_RAM mb"
	fi

	__HHUBS_NODES_MEM[$node]="$mem"
}

cdepl_cluster_node_network()
{
	local node=$1
	local net=$2

	if [ "$net" != "eth" ] && [ "$net" != "ib" ]; then
		util_log_error_and_exit "[hhubs] No instance with network $net available, supported networks: eth, ib"
	fi

	__HHUBS_NODES_NET[$node]="$net"
}

cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

	# Static IPs, no need to resolve them dynamically
	local node_id="$(echo "$hostname" | sed 's/node//')"

	if [ ! "$node_id" ]; then
		echo ""
	else
		echo "10.0.0.$node_id"
	fi
}

cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	if [ "${__HHUBS_NODE_MAP[$node]}" = "" ]; then
		util_log_error_and_exit "[hhubs][$node] Resolve node to ip failed, remote node does not exist"
	fi

	cdepl_cluster_resolve_hostname_to_ip "${__HHUBS_NODE_MAP[$node]}"
}

cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	echo "${__HHUBS_NODE_MAP[$node]}"
}

cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"
	local required_env="$3"

	if [ "${__HHUBS_NODE_MAP[$node]}" = "" ]; then
		util_log_error_and_exit "[hhubs][$node] Exec node $cmd, node does not exist"
	fi

	__cdepl_cluster_hhubs_remote_node_cmd ${__HHUBS_NODE_MAP[$node]} "$cmd"
}

cdepl_cluster_upload_to_remote()
{
	local local_path=$1
	local remote_path=$2
	local recursive=$3

	util_log_debug "[hhubs] Upload $local_path -> $remote_path"

	__cdepl_cluster_hhubs_upload_to_sollipulli $local_path $remote_path $recursive
}

cdepl_cluster_download_from_remote()
{
	local remote_path=$1
	local local_path=$2
	local recursive=$3

	util_log_debug "[hhubs] Download $remote_path -> $local_path"

	__cdepl_cluster_hhubs_download_from_sollipulli $remote_path $local_path $recursive
}

cdepl_cluster_file_system_cmd()
{
	local cmd="$1"

	# NFS is installed on hhubs -> send command to sollipulli, only
	__cdepl_cluster_hhubs_remote_sollipulli_cmd "$cmd"
}

cdepl_cluster_gather_log_files()
{
	local path="$1"
	local cur_path="$2"

	# NFS is installed on hhubs -> nothing to do here
}

cdepl_cluster_allows_sudo()
{
	echo "1"
}

####################################
# "private" callback methods that should not be called by the user

# "constructor"
_cdepl_cluster_on_init()
{
	local cluster_user=$1

	__HHUBS_SOLLIPULLI_USER="$cluster_user"

	__cdepl_cluster_hhubs_sollipulli_check

	# Force create socket for ssh connection
	# There seems to be a weird bug that the first command sent over the newly
	# created socket is silently failing
	# e.g. mkdir doesn't create the directory but doesn't error
    # Execute noop
	__cdepl_cluster_hhubs_remote_sollipulli_cmd ":"
}

# Resolve alloc'd nodes to actual physical nodes on the cluster and fill the node map with
# available hostnames
_cdepl_cluster_on_node_setup_finish()
{
	local group1_req_cnt=0
	local group2_req_cnt=0
	local group3_req_cnt=0

	local group1_avail_cnt=0
	local group2_avail_cnt=0
	local group3_avail_cnt=0

	local group1_avail=""
	local group2_avail=""
	local group3_avail=""

	# Mode 0: nodes must be pre-allocated using the cluster node script
	# Mode 1: nodes are automatically allocated using the cluster node script
	local mode=0
	local node_cmd=""

	util_log_debug "[hhubs] on_node_setup_finish"

	if [ "$mode" = "0" ]; then
		# Get all currently allocated nodes
		node_cmd="__cdepl_cluster_hhubs_remote_sollipulli_cmd \"node ls-alloc\""
	else
		# Get all free nodes and alloc what we need later
		node_cmd="__cdepl_cluster_hhubs_remote_sollipulli_cmd \"node ls-free\""
	fi

	# Sort by groups
	for node in `eval $node_cmd`; do
		if [ "$(echo $__HHUBS_NODE_GROUP_1 | grep $node)" ]; then
			group1_avail="$group1_avail $node"
			group1_avail_cnt=$(($group1_avail_cnt + 1))
		elif [ "$(echo $__HHUBS_NODE_GROUP_2 | grep $node)" ]; then
			group2_avail="$group2_avail $node"
			group2_avail_cnt=$(($group2_avail_cnt + 1))
		elif [ "$(echo $__HHUBS_NODE_GROUP_3 | grep $node)" ]; then
			group3_avail="$group3_avail $node"
			group3_avail_cnt=$(($group3_avail_cnt + 1))
		fi
	done

	util_log_debug "[hhubs] Available instances: g1($group1_avail_cnt), g2($group2_avail_cnt), g3($group3_avail_cnt)"

	local insufficient_cnt=0

	# Assign instances with IB first
	for i in `seq 0 $((__HHUBS_TOTAL_NODES - 1))`; do
		if [ "${__HHUBS_NODES_NET[$i]}" = "ib" ]; then
			if [ "${__HHUBS_NODES_CPUS[$i]}" -le "4" ] &&
					[ "${__HHUBS_NODES_MEM[$i]}" -le "32768" ] &&
					[ "$group1_req_cnt" -lt "$group1_avail_cnt" ]; then
				group1_req_cnt=$(($group1_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group1"
			elif [ "$group3_req_cnt" -lt "$group3_avail_cnt" ]; then
				group3_req_cnt=$(($group3_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group3"
			else
				insufficient_cnt=$((insufficient_cnt + 1))
			fi
		fi
	done

	# Next iteration with eth, only
	for i in `seq 0 $((__HHUBS_TOTAL_NODES - 1))`; do
		if [ "${__HHUBS_NODES_NET[$i]}" != "ib" ]; then
			if [ "${__HHUBS_NODES_CPUS[$i]}" -le "4" ] &&
					[ "${__HHUBS_NODES_MEM[$i]}" -le "16384" ] &&
					[ "$group2_req_cnt" -lt "$group2_avail_cnt" ]; then
				group2_req_cnt=$(($group2_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group2"
			elif [ "${__HHUBS_NODES_CPUS[$i]}" -le "4" ] &&
					[ "${__HHUBS_NODES_MEM[$i]}" -le "32768" ] &&
					[ "$group1_req_cnt" -lt "$group1_avail_cnt" ]; then
				group1_req_cnt=$(($group1_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group1"
			elif [ "$group3_req_cnt" -lt "$group3_avail_cnt" ]; then
				group3_req_cnt=$(($group3_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group3"
			else
				insufficient_cnt=$((insufficient_cnt + 1))
			fi
		fi
	done

	util_log_debug "[hhubs] Requested instances: g1($group1_req_cnt), g2($group2_req_cnt), g3($group3_req_cnt)"

	# Check if requested counts are available
	if [ "$insufficient_cnt" -gt "0" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of available: Lacking $insufficient_cnt instances"
	fi

	if [ "$group1_req_cnt" -gt "$group1_avail_cnt" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of group 1 available: requested $group1_req_cnt, available $group1_avail_cnt"
	fi

	if [ "$group2_req_cnt" -gt "$group2_avail_cnt" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of group 2 available: requested $group2_req_cnt, available $group2_avail_cnt"
	fi

	if [ "$group3_req_cnt" -gt "$group3_avail_cnt" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of group 3 available: requested $group3_req_cnt, available $group3_avail_cnt"
	fi

	local nodes=""

	# Generate node mappings using what's available
	# Also fill in hardware specs that are still default values with max
	# available
	for i in `seq 0 $((__HHUBS_TOTAL_NODES - 1))`; do
		# Replace marked groups to alloc with actual node hostnames
		case "${__HHUBS_NODE_MAP[$i]}" in
			"group1")
				__HHUBS_NODE_MAP[$i]=$(echo $group1_avail | cut -d " " -f1)
				group1_avail=$(echo $group1_avail | cut -d " " -f2-)

				if [ "${__HHUBS_NODES_CPUS[$i]}" = "0" ]; then
					__HHUBS_NODES_CPUS[$i]="$__HHUBS_NODE_GROUP_1_MAX_CORES"
				fi

				if [ "${__HHUBS_NODES_MEM[$i]}" = "0" ]; then
					__HHUBS_NODES_MEM[$i]="$__HHUBS_NODE_GROUP_1_MAX_RAM"
				fi

				if [ "${__HHUBS_NODES_NET[$i]}" = "" ]; then
					__HHUBS_NODES_NET[$i]="$__HHUBS_NODE_GROUP_1_MAX_NET"
				fi

				;;

			"group2")
				__HHUBS_NODE_MAP[$i]=$(echo $group2_avail | cut -d " " -f1)
				group2_avail=$(echo $group2_avail | cut -d " " -f2-)

				if [ "${__HHUBS_NODES_CPUS[$i]}" = "0" ]; then
					__HHUBS_NODES_CPUS[$i]="$__HHUBS_NODE_GROUP_2_MAX_CORES"
				fi

				if [ "${__HHUBS_NODES_MEM[$i]}" = "0" ]; then
					__HHUBS_NODES_MEM[$i]="$__HHUBS_NODE_GROUP_2_MAX_RAM"
				fi

				if [ "${__HHUBS_NODES_NET[$i]}" = "" ]; then
					__HHUBS_NODES_NET[$i]="$__HHUBS_NODE_GROUP_2_MAX_NET"
				fi

				;;

			"group3")
				__HHUBS_NODE_MAP[$i]=$(echo $group3_avail | cut -d " " -f1)
				group3_avail=$(echo $group3_avail | cut -d " " -f2-)

				if [ "${__HHUBS_NODES_CPUS[$i]}" = "0" ]; then
					__HHUBS_NODES_CPUS[$i]="$__HHUBS_NODE_GROUP_3_MAX_CORES"
				fi

				if [ "${__HHUBS_NODES_MEM[$i]}" = "0" ]; then
					__HHUBS_NODES_MEM[$i]="$__HHUBS_NODE_GROUP_3_MAX_RAM"
				fi

				if [ "${__HHUBS_NODES_NET[$i]}" = "" ]; then
					__HHUBS_NODES_NET[$i]="$__HHUBS_NODE_GROUP_3_MAX_NET"
				fi

				;;

			*)
				;;
		esac

		nodes="${nodes}${i} ${__HHUBS_NODE_MAP[$i]}\n"
	done

	util_log_debug "[hhubs] Node allocations:\n$nodes"

	# Store node mappings to file
	__cdepl_cluster_hhubs_remote_sollipulli_cmd "printf \"$nodes\" > ${__DEPLOY_CUR_OUT_PATH}/node_mappings"
}

_cdepl_cluster_before_deploy()
{
	util_log_debug "[hhubs] before_deploy"
}

_cdepl_cluster_after_deploy()
{
	util_log_debug "[hhubs] after_deploy"
}

_cdepl_cluster_before_cleanup()
{
	util_log_debug "[hhubs] before_cleanup"
}

_cdepl_cluster_after_cleanup()
{
	util_log_debug "[hhubs] after_cleanup"
}

##################################
# private to this script

__cdepl_cluster_hhubs_remote_node_cmd()
{
	local node=$1
	local cmd="$2"

	local dest=""

	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		dest="${__HHUBS_SOLLIPULLI_USER}@sollipulli"
	else
		dest="${__HHUBS_SOLLIPULLI_USER}@134.99.112.82 -p 55517"
	fi

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $dest -n -f "ssh $node \"$cmd\""
}

__cdepl_cluster_hhubs_upload_to_sollipulli()
{
	local local_path=$1
	local remote_path=$2
	local recursive=$3

	local dest=""
	local port=""

	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		dest="${__HHUBS_SOLLIPULLI_USER}@sollipulli:${remote_path}"
	else
		port="-P 55517"
		dest="${__HHUBS_SOLLIPULLI_USER}@134.99.112.82:${remote_path}"
	fi

	if [ "$recursive " ]; then
		recursive="-r"
	fi

	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $port $local_path $dest > /dev/null 2>&1
}

__cdepl_cluster_hhubs_download_from_sollipulli()
{
	local remote_path=$1
	local local_path=$2
	local recursive=$3

	local dest=""
	local port=""

	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		dest="${__HHUBS_SOLLIPULLI_USER}@sollipulli:${remote_path}"
	else
		port="-P 55517"
		dest="${__HHUBS_SOLLIPULLI_USER}@134.99.112.82:${remote_path}"
	fi

	if [ "$recursive " ]; then
		recursive="-r"
	fi

	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $port $dest $local_path > /dev/null 2>&1
}
cdepl_cluster_login_cmd()
{
	local cmd="$1"
	local required_env="$2"
	__cdepl_cluster_hhubs_remote_sollipulli_cmd 0 "$cmd"
}

__cdepl_cluster_hhubs_remote_sollipulli_cmd()
{
	local cmd="$1"

	local dest=""

	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		dest="${__HHUBS_SOLLIPULLI_USER}@sollipulli"
	else
		dest="${__HHUBS_SOLLIPULLI_USER}@134.99.112.82 -p 55517"
	fi

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $dest -n -f "$cmd"
}

__cdepl_cluster_hhubs_sollipulli_check()
{
	ping -c 1 sollipulli > /dev/null 2>&1

	# Check if sollipulli is reachable on the current net
	if [ "$?" != "0" ]; then

		# Try directly via IP
		ping -c 1 134.99.112.82 > /dev/null 2>&1

		if [ "$?" != "0" ]; then
			util_log_error_and_exit "[hhubs] Could not contact sollipulli, not available"
		else
			__HHUBS_SOLLIPULLI_MODE=2
		fi
	else
		__HHUBS_SOLLIPULLI_MODE=1
	fi

	# Check if passwordless auth available
	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		ssh -o PasswordAuthentication=no -o BatchMode=yes sollipulli exit &> /dev/null
	else
		ssh -p 55517 -o PasswordAuthentication=no -o BatchMode=yes ${__HHUBS_SOLLIPULLI_USER}@134.99.112.82 exit &> /dev/null
	fi

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hhubs] Can't connect to login node, ensure passwordless auth is set up and the cluster user is valid"
	fi

	util_log_debug "[hhubs] sollipulli available, mode $__HHUBS_SOLLIPULLI_MODE"
}

__cdepl_cluster_hhubs_alloc_hardware_specs()
{
	TODO=""
}