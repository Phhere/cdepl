#
# Copyright (C) 2018 Heinrich-Heine-Universitaet Duesseldorf, Institute of Computer Science, Department Operating Systems
#
# This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>
#

#!/bin/bash

#
# Cluster module for HILBERT HPC system of the Heinrich-Heine-University
# Duesseldorf
#

readonly __HILBERT_MAX_NODES="112"
readonly __HILBERT_MAX_CPUS_PER_NODE="24"
readonly __HILBERT_MAX_MEM_PER_NODE="106903"
readonly __HILBERT_HOSTNAME="hpc.rz.uni-duesseldorf.de"

readonly __HILBERT_QSUB="/opt/pbs/bin/qsub"
readonly __HILBERT_QDEL="/opt/pbs/bin/qdel"

readonly __PBS_ATTACH="1"

__HILBERT_NODE_EXCL_FIRST_CALL=""

__HILBERT_TOTAL_NODES="0"
__HILBERT_CPUS_PER_NODE="0"
__HILBERT_MEM_MB_PER_NODE="0"

__HILBERT_USER=""
__HILBERT_PROJECT=""
__HILBERT_WALLTIME=""
__HILBERT_JOB_NAME=""
__HILBERT_QUEUE=""
__HILBERT_NODES_EXPLICIT=""
__HILBERT_NODES_SINGLE_SOCKET=()

__HILBERT_JOB_ID=""
__HILBERT_JOB_ID_LONG=""
__HILBERT_NODE_MAPPING=()

declare -gA __HILBERT_DEPENDENCY_MAP
__HILBERT_DEPENDENCY_MAP["boost/1.55.0"]="Boost/1.55.0"
__HILBERT_DEPENDENCY_MAP["boost/1.62.0"]="Boost/1.62.0"
__HILBERT_DEPENDENCY_MAP["gcc/4.9.4"]="gcc/4.9.4"
__HILBERT_DEPENDENCY_MAP["gcc/6.1"]="gcc/6.1.0"
__HILBERT_DEPENDENCY_MAP["gcc/6.1.0"]="gcc/6.1.0"
__HILBERT_DEPENDENCY_MAP["java/1.8"]="Java/1.8.0_151"
__HILBERT_DEPENDENCY_MAP["java/1.8.0"]="Java/1.8.0_151"
__HILBERT_DEPENDENCY_MAP["pcre/8.38.0"]="pcre/8.38"
__HILBERT_DEPENDENCY_MAP["protobuf/2.6.1"]="protobuf/2.6.1"

cdepl_cluster_get_base_path_deploy_out()
{
	local user=$1

    # Use gpfs instead of home -> more space
	echo "/gpfs/project/$user"
}

cdepl_cluster_node_alloc()
{
	local num_nodes=$1

	if [ "$num_nodes" -gt "$__HILBERT_MAX_NODES" ]; then
		util_log_error_and_exit "[hilbert] Cannot allocate $num_nodes, max: $__HILBERT_MAX_NODES"
	fi

	__HILBERT_TOTAL_NODES="$num_nodes"
}

cdepl_cluster_get_alloc_node_count()
{
	echo "$__HILBERT_TOTAL_NODES"
}

cdepl_cluster_node_excl()
{
	local node=$1

	# Max out for all instances, I don't think it's possible to allocate
	# different amounts of cpu and ram for different instances
	__HILBERT_CPUS_PER_NODE="$__HILBERT_MAX_CPUS_PER_NODE"
	__HILBERT_MEM_MB_PER_NODE="$__HILBERT_MAX_MEM_PER_NODE"

	# Print this once
	if [ ! "$__HILBERT_NODE_EXCL_FIRST_CALL" ]; then
		__HILBERT_NODE_EXCL_FIRST_CALL="0"
		util_log "All nodes ($__HILBERT_TOTAL_NODES) allocated exclusively. Depending on the number of nodes selected, your job might have to wait a little longer until you get all the nodes exclusively"
	fi
}

cdepl_cluster_node_cpu_limit_single_socket()
{
	local node=$1

	__HILBERT_NODES_SINGLE_SOCKET[$node]="1"
}

cdepl_cluster_node_cpus()
{
	local node=$1
	local cpus=$2

	# I don't think it's possible to allocate a different number of cpus
	# per instance
	# Just use the highest number and allocate that amount on every node
	if [ "$cpus" -gt "$__HILBERT_CPUS_PER_NODE" ] && [ "$cpus" -le "$__HILBERT_MAX_CPUS_PER_NODE" ]; then
		__HILBERT_CPUS_PER_NODE="$cpus"
	fi
}

cdepl_cluster_node_mem()
{
	local node=$1
	local mem=$2

	if [ "$mem" -gt "$__HILBERT_MEM_MB_PER_NODE" ] && [ "$mem" -le "$__HILBERT_MAX_MEM_PER_NODE" ]; then
		__HILBERT_MEM_MB_PER_NODE="$mem"
	fi
}

cdepl_cluster_node_network()
{
	local node=$1
	local net=$2

	# All nodes are supporting eth and ib
}

cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

	echo "$(__cdepl_cluster_login_cmd "getent hosts $hostname | sed -E 's/([0-9]*.[0-9]*.[0-9]*.[0-9]*).*/\1/p' | head -n 1")"
}

cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	cdepl_cluster_resolve_hostname_to_ip "${__HILBERT_NODE_MAPPING[$node]}"
}

cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	echo "${__HILBERT_NODE_MAPPING[$node]}"
}

cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"
	local required_env="$3"

	local env=""

	if [ "$required_env" ]; then
		local hilbert_env=""

		for entry in $required_env; do
			local hilbert_dep=${__HILBERT_DEPENDENCY_MAP[$entry]}

			if [ ! "$hilbert_dep" ]; then
				util_log_error_and_exit "[hilbert][$node] Could not resolve environment $entry, no mapping available"
			fi

			hilbert_env="$hilbert_env $hilbert_dep"
		done

		env="module load $hilbert_env ; "
	fi

    local taskset=""
    local inner_command=""
    local complete_command=""
    local ssh_wrapped=""

    printf -v cmd '%q ' $cmd
    # Limit to single socket?
    if [ "${__HILBERT_NODES_SINGLE_SOCKET[$node]}" ]; then
        printf -v inner_command '%q ' $env taskset -c 0-11 bash -c "\"$cmd\""
    else
		printf -v inner_command '%q ' $env $cmd
    fi

    # Run inside pbs_attach for resource monitoring
    if [ "$__PBS_ATTACH" == "1" ]; then
		printf -v complete_command '%q ' pbs_attach -P -s -j $__HILBERT_JOB_ID_LONG bash -c "\"$inner_command\""
	else
		complete_command=$inner_command
	fi

	printf -v ssh_wrapped '%q ' ssh -q ${__HILBERT_NODE_MAPPING[$node]} bash -c "\"$complete_command\""

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ServerAliveInterval=60 -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME} -n -f bash -c "\"$ssh_wrapped\""
}

cdepl_cluster_allows_sudo()
{
	echo ""
}

cdepl_cluster_upload_to_remote()
{
	local local_path=$1
	local remote_path=$2
	local recursive=$3

	if [ "$recursive" ]; then
		recursive="-r"
	fi

	util_log_debug "[hilbert] Upload $local_path -> $remote_path"

	# Use ControlMaster to establish the TCP connection, once, and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $local_path ${__HILBERT_USER}@${__HILBERT_HOSTNAME}:${remote_path} > /dev/null 2>&1
}

cdepl_cluster_download_from_remote()
{
	local remote_path=$1
	local local_path=$2
	local recursive=$3

	if [ "$recursive" ]; then
		recursive="-r"
	fi

	util_log_debug "[hilbert] Download $remote_path -> $local_path"

	# Use ControlMaster to establish the TCP connection, once, and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME}:$remote_path $local_path > /dev/null 2>&1
}

cdepl_cluster_file_system_cmd()
{
	local cmd="$1"

	# We use a scratch file system for hilbert -> send command to login node
	__cdepl_cluster_login_cmd "$cmd"
}

cdepl_cluster_gather_log_files()
{
    local path="$1"
    local cur_path="$2"

	# We use a scratch file system for hilbert -> nothing to do here
}

####################################
# "private" callback methods that should not be called by the user

# "constructor"
_cdepl_cluster_on_init()
{
	local cluster_user=$1
	# Required for job system
	local project_name=$2
	# Also required for job system
	local walltime=$3

	# Default walltime 30 mins if not specified
	if [ ! "$walltime" ]; then
		walltime="00:30:00"
	fi

	__HILBERT_WALLTIME="$walltime"

    # Optional arguments, e.g.
    # q=Devel
    # j=bla
    # n=hilbert74+hilbert75+hilbert76
    local job_name=""
    local queue=""
    local nodes=""

    for arg in "${@:4}"; do
        case "${arg:0:2}" in
            "j=") job_name="${arg:2}" ;;
            "q=") queue="${arg:2}" ;;
            "n=") nodes="${arg:2}" ;;
            *) ;;
        esac
    done

	util_log_debug "[hilbert] on_init: user $cluster_user project $project_name $walltime"

	__HILBERT_USER="$cluster_user"

	if [ ! "$project_name" ]; then
		util_log_error_and_exit "[hilbert] No project name specified on init"
	fi

	__HILBERT_PROJECT="$project_name"

	# If no job name specified, default
	if [ "$job_name" ]; then
		__HILBERT_JOB_NAME="$job_name"
	else
		__HILBERT_JOB_NAME="${__HILBERT_PROJECT}"
	fi

    __HILBERT_QUEUE="$queue"
    __HILBERT_NODES_EXPLICIT="$nodes"

	# Check if passwordless auth available
	ssh -o PasswordAuthentication=no -o BatchMode=yes ${__HILBERT_USER}@${__HILBERT_HOSTNAME} exit &> /dev/null

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hilbert] Can't connect to login node, ensure passwordless auth is set up and the cluster user is valid"
	fi

	# Force create socket for ssh connection
	# There seems to be a weird bug that the first command sent over the newly
	# created socket is silently failing
	# e.g. mkdir doesn't create the directory but doesn't error
    # Execute noop
	__cdepl_cluster_login_cmd ":"
}

_cdepl_cluster_on_node_setup_finish()
{
	util_log_debug "[hilbert] on_node_setup_finish"

	# Create job script to bootstrap the cluster: allocate nodes, get node mappings

	local tmp_job_script="/tmp/cdepl_hilbert_deploy.job"
	local job_script="${__DEPLOY_CUR_OUT_PATH}/deploy.job"
	local node_mappings_path="${__DEPLOY_CUR_OUT_PATH}/node_mappings"

    # We could pass this stuff as parameters to PBS but want them documented
    # with the job script
    local pbs_job_string=""

    if [ "$__HILBERT_NODES_EXPLICIT" ]; then
        pbs_job_string="#PBS -l nodes=${__HILBERT_NODES_EXPLICIT}"
        util_log_debug "[hilbert] Explicit node allocation: ${__HILBERT_NODES_EXPLICIT}"
    else
        pbs_job_string="#PBS -l select=${__HILBERT_TOTAL_NODES}:ncpus=${__HILBERT_CPUS_PER_NODE}:mem=${__HILBERT_MEM_MB_PER_NODE}MB:arch=ivybridge
#PBS -l place=scatter"
    fi

    if [ "$__HILBERT_QUEUE" ]; then
        pbs_job_string="$pbs_job_string
#PBS -q ${__HILBERT_QUEUE}"
        util_log_debug "[hilbert] Non default queue specified: ${__HILBERT_QUEUE}"
    fi

    pbs_job_string="$pbs_job_string
#PBS -l walltime=${__HILBERT_WALLTIME}
#PBS -r n
#PBS -N $__HILBERT_JOB_NAME
#PBS -A $__HILBERT_PROJECT
#PBS -e ${__DEPLOY_CUR_OUT_PATH}/deploy.stderr
#PBS -o ${__DEPLOY_CUR_OUT_PATH}/deploy.stdout"

    # Glue everything together
	printf '%s' "#!/bin/bash

${pbs_job_string}

JOB_WORKING_DIR=\"${__DEPLOY_CUR_OUT_PATH}\"
NODES_COUNT=\"\"
NODES_HOSTNAME=\"\"
NODE_MAPPINGS=\"\"

echo \"Getting reserved nodes...\"

NODES_COUNT=0
for NODE in \$(cat \$PBS_NODEFILE); do
	NODES_HOSTNAME[\$NODES_COUNT]=\$NODE
	NODES_COUNT=\$[NODES_COUNT + 1]
done

echo \"Total node count: \$NODES_COUNT\"

echo \"Resolving node mappings for deployment: \"${__DEPLOY_CUR_OUT_PATH}\"\"

i=0
while [ \$i -lt \$NODES_COUNT ]; do
	NODE_MAPPINGS=\"\${NODE_MAPPINGS}\${i} \${NODES_HOSTNAME[i]}\\n\"
	i=\$[i + 1]
done

printf \"\$NODE_MAPPINGS\" > \"$node_mappings_path\"
sync

# Keep job running to keep allocated nodes active
run_time=\$(echo \"${__HILBERT_WALLTIME}\" | awk -F: '{ print (\$1 * 3600) + (\$2 * 60) + \$3 }')

echo \"Keep job running for \$run_time seconds...\"
sleep \$run_time

echo \"Job finished\"
" > $tmp_job_script

	sync

	scp $tmp_job_script ${__HILBERT_USER}@${__HILBERT_HOSTNAME}:$job_script > /dev/null 2>&1
	__cdepl_cluster_login_cmd "chmod +x $job_script"

    # A little late but print which resources are currently available. That
    # might give the user a hint if the job can start very soon. Maybe he
    # decides to reduce the resource needs as well to get something started
    # quickly

    local resource_available="$(curl -k "https://myjam3.hhu.de/ajax.php?modul=cluster&script=FreeRessources" 2> /dev/null)"

    util_log_debug "[hilbert] Resources currently available:\n${resource_available}"

	# Submit the job script and wait for the node mappings to be written to disk

	local qsub_output=$(__cdepl_cluster_login_cmd "$__HILBERT_QSUB $job_script")

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hilbert] Submitting job script $job_script failed"
	fi

	# Wait for node_mappings file to appear

	__HILBERT_JOB_ID=$(echo $qsub_output | cut -d '.' -f 1)
	__HILBERT_JOB_ID_LONG=$qsub_output

	util_log_debug "[hilbert] Job submitted $__HILBERT_JOB_ID ($qsub_output), waiting for job to start..."

	local node_mappings=""

	# ... and get the node mappings assigned by the job system

    local start_time=$(date +%s)
    local diff_time=""
    local time_str="00:00:00"

    echo "Wait time elapsed: 00:00:00"

	while true; do
		# Hack to trigger cache flush of gpfs to avoid waiting longer than
		# necessary (approx 3 sec compared to 30 sec)
		__cdepl_cluster_login_cmd "ls $__DEPLOY_CUR_OUT_PATH > /dev/null 2>&1"

		if [ "$(__cdepl_cluster_login_cmd "[ -f ${node_mappings_path} ] && echo 1")" = "1" ]; then
			node_mappings=$(__cdepl_cluster_login_cmd "cat $node_mappings_path")
			
			while read -r line; do
				local id=$(echo "$line" | cut -d ' ' -f 1)
				local hostname=$(echo "$line" | cut -d ' ' -f 2)

				__HILBERT_NODE_MAPPING[$id]="$hostname"
			done <<< "$node_mappings"

			break
		fi

		sleep 1
        diff_time=$(date +%s)
		diff_time=$((diff_time - start_time))
        time_str=$(printf "%02d:%02d:%02d" $((diff_time / 3600)) $(((diff_time / 60) % 60)) $((diff_time % 60)))

        # Overwrite previous time
        echo -en "\e[1A"; echo -e "\e[0K\rWait time elapsed: $time_str"
	done

	echo ""

	util_log_debug "[hilbert] Job started, node allocations:\n${node_mappings}"
}

_cdepl_cluster_before_deploy()
{
	util_log_debug "[hilbert] before_deploy"
}

_cdepl_cluster_after_deploy()
{
	util_log_debug "[hilbert] after_deploy"
}

_cdepl_cluster_before_cleanup()
{
	util_log_debug "[hilbert] before_cleanup"
}

_cdepl_cluster_after_cleanup()
{
	util_log_debug "[hilbert] after_cleanup"

	util_log_debug "[hilbert] Deployment finished, delete job $__HILBERT_JOB_ID"
	__cdepl_cluster_login_cmd "qdel $__HILBERT_JOB_ID" > /dev/null 2>&1
}

__cdepl_cluster_login_cmd()
{
	local cmd="$1"
    local required_env="$2"

    local env=""

    if [ "$required_env" ]; then
        local hilbert_env=""

        for entry in $required_env; do
            local hilbert_dep=${__HILBERT_DEPENDENCY_MAP[$entry]}

            if [ ! "$hilbert_dep" ]; then
                util_log_error_and_exit "[hilbert][$node] Could not resolve environment $entry, no mapping available"
            fi

            hilbert_env="$hilbert_env $hilbert_dep"
        done

        env="module load $hilbert_env ; "
    fi

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds 
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ServerAliveInterval=60 -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME} -n -f "$env $cmd"
}